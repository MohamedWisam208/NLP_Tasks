{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Preprocessing:**"
      ],
      "metadata": {
        "id": "9juHyOafF2pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1- Tokenization**"
      ],
      "metadata": {
        "id": "xKSM4n1y-ow2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word-based tokenization"
      ],
      "metadata": {
        "id": "06d_rU4t-2LY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ByGqYit-h2j",
        "outputId": "0b7d8969-5ada-4cfe-9302-e7cd3129e606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens: ['Artificial', 'intelligence', 'is', 'the', 'future', '.', 'Artificial', 'intelligence', 'improves', 'human', 'life', '.', 'The', 'future', 'of', 'Artificial', 'intelligence', 'is', 'bright', '.']\n",
            "Arabic Tokens: ['الذكاء', 'الاصطناعي', 'هو', 'المستقبل', '.', 'الذكاء', 'الاصطناعي', 'يعزز', 'حياة', 'الإنسان', '.', 'المستقبل', 'الذكاء', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load language models for English and Arabic\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_ar = spacy.blank(\"ar\")  # spaCy supports Arabic tokenization, but tagging requires more tools\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "# Example texts\n",
        "text_en =  \"Artificial intelligence is the future. Artificial intelligence improves human life. The future of Artificial intelligence is bright.\"\n",
        "text_ar =  \"الذكاء الاصطناعي هو المستقبل. الذكاء الاصطناعي يعزز حياة الإنسان. المستقبل الذكاء.\"\n",
        "\n",
        "tokens_en = tokenize_text(text_en, nlp_en)\n",
        "tokens_ar = tokenize_text(text_ar, nlp_ar)\n",
        "\n",
        "print(\"English Tokens:\", tokens_en)\n",
        "print(\"Arabic Tokens:\", tokens_ar)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "character-based tokenization"
      ],
      "metadata": {
        "id": "6mPqke5d_BVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C_tokens_arabic  = [char for char in text_ar]\n",
        "C_tokens_english = [char for char in text_en]\n",
        "\n",
        "# Display tokens\n",
        "print(\"Tokens for Arabic Text:\", C_tokens_arabic)\n",
        "print(\"Tokens for English Text:\", C_tokens_english)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8KKKAXdAl7P",
        "outputId": "bf1609fc-6b41-4ddb-e6ad-c8b0660d5a03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens for Arabic Text: ['ا', 'ل', 'ذ', 'ك', 'ا', 'ء', ' ', 'ا', 'ل', 'ا', 'ص', 'ط', 'ن', 'ا', 'ع', 'ي', ' ', 'ه', 'و', ' ', 'ا', 'ل', 'م', 'س', 'ت', 'ق', 'ب', 'ل', '.', ' ', 'ا', 'ل', 'ذ', 'ك', 'ا', 'ء', ' ', 'ا', 'ل', 'ا', 'ص', 'ط', 'ن', 'ا', 'ع', 'ي', ' ', 'ي', 'ع', 'ز', 'ز', ' ', 'ح', 'ي', 'ا', 'ة', ' ', 'ا', 'ل', 'إ', 'ن', 'س', 'ا', 'ن', '.', ' ', 'ا', 'ل', 'م', 'س', 'ت', 'ق', 'ب', 'ل', ' ', 'ا', 'ل', 'ذ', 'ك', 'ا', 'ء', '.']\n",
            "Tokens for English Text: ['A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r', 'e', '.', ' ', 'A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'i', 'm', 'p', 'r', 'o', 'v', 'e', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'i', 'f', 'e', '.', ' ', 'T', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r', 'e', ' ', 'o', 'f', ' ', 'A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'b', 'r', 'i', 'g', 'h', 't', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2- Stopword Removal**"
      ],
      "metadata": {
        "id": "mxVhvsofAmVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English and Arabic stopword removal\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as EN_STOPWORDS\n",
        "from spacy.lang.ar.stop_words import STOP_WORDS as AR_STOPWORDS\n",
        "\n",
        "def remove_stopwords(tokens, stopwords):\n",
        "    return [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "filtered_en = remove_stopwords(tokens_en, EN_STOPWORDS)\n",
        "filtered_ar = remove_stopwords(tokens_ar, AR_STOPWORDS)\n",
        "\n",
        "print(\"Filtered English Tokens:\", filtered_en)\n",
        "print(\"Filtered Arabic Tokens:\", filtered_ar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNP4xl_1_G0E",
        "outputId": "40a9f92d-ca47-4ecd-d74c-1d40d8b55a27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered English Tokens: ['Artificial', 'intelligence', 'future', '.', 'Artificial', 'intelligence', 'improves', 'human', 'life', '.', 'future', 'Artificial', 'intelligence', 'bright', '.']\n",
            "Filtered Arabic Tokens: ['الذكاء', 'الاصطناعي', 'المستقبل', '.', 'الذكاء', 'الاصطناعي', 'يعزز', 'حياة', 'الإنسان', '.', 'المستقبل', 'الذكاء', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3- Noice Removal**"
      ],
      "metadata": {
        "id": "YQu55YrEDa39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuation = string.punctuation\n",
        "\n",
        "cleaned_ar = [word for word in filtered_ar if word not in punctuation]\n",
        "cleaned_en = [word for word in filtered_en if word not in punctuation]\n",
        "\n",
        "# showing The Punctuation\n",
        "print(\"Punctuation Elements :\",punctuation)\n",
        "\n",
        "# Display tokens after removing punctuation\n",
        "print(\"\\n\\nFiltered Arabic Tokens without Punctuation:\", cleaned_ar)\n",
        "print(\"\\nFiltered English Tokens without Punctuation:\", cleaned_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmM3Pqj_D7hu",
        "outputId": "e4db5d01-70c2-4e9c-a071-4a6ee5749e99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation Elements : !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "\n",
            "Filtered Arabic Tokens without Punctuation: ['الذكاء', 'الاصطناعي', 'المستقبل', 'الذكاء', 'الاصطناعي', 'يعزز', 'حياة', 'الإنسان', 'المستقبل', 'الذكاء']\n",
            "\n",
            "Filtered English Tokens without Punctuation: ['Artificial', 'intelligence', 'future', 'Artificial', 'intelligence', 'improves', 'human', 'life', 'future', 'Artificial', 'intelligence', 'bright']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4- Normalization**"
      ],
      "metadata": {
        "id": "_i5w4IEEDdQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization function\n",
        "def normalize_text(tokens, is_arabic=False):\n",
        "    if is_arabic:\n",
        "        # Normalize Arabic letters\n",
        "        normalized = [token.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\") for token in tokens]\n",
        "    else:\n",
        "        normalized = tokens\n",
        "    # Convert to lowercase\n",
        "    normalized = [token.lower() for token in normalized]\n",
        "    return normalized\n",
        "\n",
        "# Normalization\n",
        "normalized_en = normalize_text(cleaned_en)\n",
        "normalized_ar = normalize_text(cleaned_ar, is_arabic=True)\n",
        "\n",
        "print(\"Normalized English Tokens:\", normalized_en)\n",
        "print(\"Normalized Arabic Tokens:\", normalized_ar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2_bvyPBEqpq",
        "outputId": "177e7f55-73ba-420e-a276-832772ef3c57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized English Tokens: ['artificial', 'intelligence', 'future', 'artificial', 'intelligence', 'improves', 'human', 'life', 'future', 'artificial', 'intelligence', 'bright']\n",
            "Normalized Arabic Tokens: ['الذكاء', 'الاصطناعي', 'المستقبل', 'الذكاء', 'الاصطناعي', 'يعزز', 'حياة', 'الانسان', 'المستقبل', 'الذكاء']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5- POS Tagging**"
      ],
      "metadata": {
        "id": "CvJK4sHkDeaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF_Ry4VrH6gp",
        "outputId": "2fcb809b-6bcf-4ea3-cddd-01006bb194d0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camel-tools in /usr/local/lib/python3.10/dist-packages (1.5.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.16.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from camel-tools) (5.5.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.5.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.3.9)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers<4.44.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (4.43.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.32.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.14.0)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.20.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from camel-tools) (4.66.6)\n",
            "Requirement already satisfied: muddler in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.1.3)\n",
            "Requirement already satisfied: camel-kenlm>=2024.5.6 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2024.5.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m camel_tools.download.downloader --url https://download.camel-tools.org/calima-msa-r13.zip --output /tmp/calima-msa-r13.zip\n",
        "!unzip /tmp/calima-msa-r13.zip -d /tmp/calima-msa-r13\n",
        "!python -m camel_tools register_builtin_db --db-path /tmp/calima-msa-r13/calima-msa-r13-db --db-name calima-msa-r13 --force\n",
        "!camel_data -i morphology-db-msa-s31 # Install the required morphological database\n",
        "!camel_data -i disambig-bert-unfactored-msa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMjkiFaBI1tS",
        "outputId": "fca790df-dd6a-45a8-88eb-ed57bcd31811"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'camel_tools.download.downloader' (ModuleNotFoundError: No module named 'camel_tools.download')\n",
            "unzip:  cannot find or open /tmp/calima-msa-r13.zip, /tmp/calima-msa-r13.zip.zip or /tmp/calima-msa-r13.zip.ZIP.\n",
            "/usr/bin/python3: No module named camel_tools.__main__; 'camel_tools' is a package and cannot be directly executed\n",
            "No new packages will be installed.\n",
            "No new packages will be installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English POS Tagging\n",
        "def pos_tagging_english(tokens, nlp_model):\n",
        "    doc = nlp_model(\" \".join(tokens))\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "pos_tags_en = pos_tagging_english(normalized_en, nlp_en)\n",
        "\n",
        "# Arabic POS Tagging using Camel Tools (requires installation)\n",
        "# pip install camel-tools\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.morphology.database import MorphologyDB\n",
        "\n",
        "# Initialize MorphologyDB with the desired database\n",
        "db = MorphologyDB.builtin_db('calima-msa-r13')\n",
        "# Initialize Analyzer with the MorphologyDB instance\n",
        "analyzer = Analyzer(db=db)\n",
        "def pos_tag_arabic(tokens):\n",
        "    pos_tags = []\n",
        "    for token in tokens:\n",
        "        analyses = analyzer.analyze(token)\n",
        "        if analyses:\n",
        "            pos_tags.append((token, analyses[0]['pos']))\n",
        "        else:\n",
        "            pos_tags.append((token, 'unknown'))\n",
        "    return pos_tags\n",
        "\n",
        "pos_tags_ar = pos_tag_arabic(normalized_ar)\n",
        "\n",
        "print(\"English POS Tags:\", pos_tags_en)\n",
        "print(\"Arabic POS Tags:\", pos_tags_ar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AciiRMfQGe1P",
        "outputId": "2a40b441-fd1f-422a-aca3-40e21013c56b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English POS Tags: [('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('future', 'ADJ'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('improves', 'VERB'), ('human', 'ADJ'), ('life', 'NOUN'), ('future', 'ADJ'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('bright', 'ADJ')]\n",
            "Arabic POS Tags: [('الذكاء', 'noun'), ('الاصطناعي', 'adj'), ('المستقبل', 'noun'), ('الذكاء', 'noun'), ('الاصطناعي', 'adj'), ('يعزز', 'verb'), ('حياة', 'verb'), ('الانسان', 'noun'), ('المستقبل', 'noun'), ('الذكاء', 'noun')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Analysis and Interpretation**\n",
        "\n",
        "After applying the preprocessing steps, the text undergoes significant transformations that improve its quality and readiness for further analysis or modeling. Here's a breakdown of how each step contributes to this improvement:\n",
        "\n",
        "### **1. Tokenization**\n",
        "- **Changes in Text**: The input text is divided into smaller units, typically words or sentences. For example:  \n",
        "  - **English**: `\"The weather is lovely today, isn't it?\"` → `['The', 'weather', 'is', 'lovely', 'today', ',', \"isn't\", 'it', '?']`  \n",
        "  - **Arabic**: `\"الطقس جميل اليوم، أليس كذلك؟\"` → `['الطقس', 'جميل', 'اليوم', '،', 'أليس', 'كذلك', '؟']`\n",
        "- **Benefits**:\n",
        "  - Converts the text into manageable pieces for analysis.\n",
        "  - Essential for downstream tasks like stopword removal, tagging, and feature extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Stopword Removal**\n",
        "- **Changes in Text**: Common, non-informative words are removed.  \n",
        "  - **English**: `['The', 'weather', 'is', 'lovely', 'today']` → `['weather', 'lovely', 'today']`  \n",
        "  - **Arabic**: `['الطقس', 'جميل', 'اليوم']` → `['جميل', 'اليوم']`\n",
        "- **Benefits**:\n",
        "  - Reduces text dimensionality by eliminating filler words.\n",
        "  - Helps focus on content-rich words that carry meaningful information.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Noise Removal**\n",
        "- **Changes in Text**: Symbols, numbers, and unnecessary punctuation are stripped away.  \n",
        "  - **English**: `['weather', 'lovely', 'today', ',']` → `['weather', 'lovely', 'today']`  \n",
        "  - **Arabic**: `['جميل', 'اليوم', '؟']` → `['جميل', 'اليوم']`\n",
        "- **Benefits**:\n",
        "  - Cleans up irrelevant elements that could interfere with analysis.\n",
        "  - Makes the text more uniform and interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Normalization**\n",
        "- **Changes in Text**: Text is standardized:  \n",
        "  - **English**: Converts all characters to lowercase for uniformity.  \n",
        "    Example: `['Lovely', 'Today']` → `['lovely', 'today']`  \n",
        "  - **Arabic**: Standardizes letter forms (e.g., \"أ\" → \"ا\") and removes extra spaces.  \n",
        "    Example: `['أليس', 'كذلك']` → `['اليس', 'كذلك']`\n",
        "- **Benefits**:\n",
        "  - Ensures consistency in the data.\n",
        "  - Reduces variability caused by different cases or spelling differences, especially in Arabic.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. POS Tagging**\n",
        "- **Changes in Text**: Adds grammatical labels to each token:  \n",
        "  - **English**: `['weather', 'lovely', 'today']` → `[('weather', 'NOUN'), ('lovely', 'ADJ'), ('today', 'NOUN')]`  \n",
        "  - **Arabic**: `['جميل', 'اليوم']` → `[('جميل', 'ADJ'), ('اليوم', 'NOUN')]`\n",
        "- **Benefits**:\n",
        "  - Provides grammatical context, enabling tasks like dependency parsing and syntactic analysis.\n",
        "  - Helps differentiate between word meanings (e.g., \"play\" as a noun vs. verb).\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Benefits**\n",
        "The preprocessing steps collectively enhance the text by:\n",
        "- **Simplifying the structure**: Tokenization and stopword removal reduce complexity and highlight relevant content.\n",
        "- **Cleaning and standardizing**: Noise removal and normalization produce cleaner, uniform text.\n",
        "- **Providing linguistic insights**: POS tagging adds syntactic and semantic value, which is essential for feature engineering in NLP tasks.\n",
        "\n",
        "This processed text is now more suitable for tasks like text classification, sentiment analysis, or predictive modeling, as irrelevant details are minimized and the focus is shifted to meaningful patterns."
      ],
      "metadata": {
        "id": "5Ro2EUI1KxN-"
      }
    }
  ]
}